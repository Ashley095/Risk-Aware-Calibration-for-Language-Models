# -*- coding: utf-8 -*-
"""Language Model Calibration Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qfl1Pa8uZcFMRsLxLUyDM8LXqfwZLr8i

# Part 1: Data Preparation and Model Scoring Framework

This section handles dataset creation, loading, and defines model-agnostic scoring interfaces for multiple architectures (DeBERTa-NLI, FLAN-T5, RoBERTa, Phi-3.5).

## Contents:
1. Library imports and global configuration
2. HellaSwag balanced subset creation
3. Data loading utilities
4. Model scorer base class and implementations
5. Data splitting utilities
"""

import os, random, csv, json, math, re
from collections import Counter, defaultdict
from typing import List
from math import sqrt

import numpy as np
import pandas as pd
from datasets import load_dataset
from tqdm import tqdm

import torch
import matplotlib.pyplot as plt

from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    T5ForConditionalGeneration, T5TokenizerFast,
    RobertaForMultipleChoice, RobertaTokenizerFast,
    AutoModelForCausalLM
)

from sklearn.metrics import log_loss
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.isotonic import IsotonicRegression

# 0) GLOBAL CONFIGURATION
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# Directory for shared data (HellaSwag subset)
DATA_DIR = "data"
# Parent directory for all model-specific results
RESULTS_DIR = "all_model_runs"

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Configs for HellaSwag subset creation
HS_SPLIT = "validation"
HS_TARGET_N = 1000
HS_PER_CLASS_TARGET = HS_TARGET_N // 4
HS_ALLOW_DUP_QUESTION = False

# Configs for data loading (now points to DATA_DIR)
DATA_PATHS = [
    os.path.join(DATA_DIR, "hellaswag_subset.csv"),
    os.path.join(DATA_DIR, "hellaswag_subset.jsonl"),
]
MAX_N_LOAD = 1000 # Max items to *load* from the subset

# PART 1: HELLASWAG SUBSET CREATION

def parse_label(v):
    """Convert raw label to {0,1,2,3}; return None if invalid."""
    if v is None: return None
    s = str(v).strip()
    if s.lstrip("-").isdigit():
        k = int(s)
        return k if k in (0, 1, 2, 3) else None
    return None

def get_question(ex):
    """Extract the question text in a unified way."""
    q = ex.get("ctx")
    if q and len(str(q).strip()) > 0:
        return str(q)
    qa, qb = ex.get("ctx_a"), ex.get("ctx_b")
    parts = [p for p in (qa, qb) if p is not None and len(str(p).strip()) > 0]
    return " ".join(map(str, parts))

def get_endings(ex):
    """Extract the 4 answer options in a unified way."""
    ends = ex.get("endings")
    if isinstance(ends, (list, tuple)) and len(ends) == 4:
        return [str(x) for x in ends]
    return None

def create_hellaswag_subset():
    """
    Create a balanced HellaSwag subset and save CSV / JSONL into DATA_DIR.
    """
    print("=" * 60)
    print(f"Creating HellaSwag subset (target {HS_TARGET_N} items)...")
    print("=" * 60)

    # 1) Read HellaSwag
    ds = load_dataset("Rowan/hellaswag", split=HS_SPLIT)
    print(f"[load] HellaSwag split={HS_SPLIT}, rows={len(ds)}")

    # 2) Parse into a unified pool
    pool = []
    for ex in ds:
        q = get_question(ex)
        opts = get_endings(ex)
        y = parse_label(ex.get("label"))
        if not q or not isinstance(opts, list) or len(opts) != 4 or y is None:
            continue
        if any(len(o.strip()) == 0 for o in opts):
            continue
        pool.append({"q": q, "opts": opts, "y": y})

    print(f"[parsed] usable items: {len(pool)}")
    cnt = Counter(p["y"] for p in pool)
    print("label counts (0..3):", [cnt.get(i, 0) for i in range(4)])

    # 3) Deduplicate by question text if needed
    if not HS_ALLOW_DUP_QUESTION:
        seen = set()
        uniq = []
        for r in pool:
            key = r["q"].strip()
            if key in seen:
                continue
            seen.add(key)
            uniq.append(r)
        pool = uniq
        print(f"[dedup question] remain: {len(pool)}")

    # 4) Balanced sampling by class
    by_cls = defaultdict(list)
    for r in pool:
        by_cls[r["y"]].append(r)
    for c in range(4):
        random.shuffle(by_cls[c])

    per_class = min(HS_PER_CLASS_TARGET, min(len(by_cls[c]) for c in range(4)))
    picked = []
    for c in range(4):
        take = min(per_class, len(by_cls[c]))
        picked.extend(by_cls[c][:take])

    remaining = [r for r in pool if r not in picked]
    random.shuffle(remaining)
    need = max(0, HS_TARGET_N - len(picked))
    picked.extend(remaining[:need])
    picked = picked[:HS_TARGET_N]

    cnt_sub = Counter(r["y"] for r in picked)
    print("[subset] counts:", [cnt_sub.get(i, 0) for i in range(4)], "| total =", len(picked))

    # 5) Export CSV (to DATA_DIR)
    csv_path = os.path.join(DATA_DIR, "hellaswag_subset.csv")
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["y(0..3)", "answer_letter", "question", "optA", "optB", "optC", "optD"])
        for r in picked:
            w.writerow([r["y"], "ABCD"[r["y"]], r["q"], *r["opts"]])
    print("Saved:", csv_path)

    # 6) Export JSONL (to DATA_DIR)
    jsonl_path = os.path.join(DATA_DIR, "hellaswag_subset.jsonl")
    with open(jsonl_path, "w", encoding="utf-8") as f:
        for r in picked:
            f.write(
                json.dumps(
                    {
                        "label": r["y"],
                        "answer_letter": "ABCD"[r["y"]],
                        "question": r["q"],
                        "options": r["opts"],
                    },
                    ensure_ascii=False,
                )
                + "\n"
            )
    print("Saved:", jsonl_path)
    return picked

# PART 2: DATA LOADING

def load_items_from_csv(path):
    df = pd.read_csv(path)
    ycol = "y(0..3)" if "y(0..3)" in df.columns else ("y" if "y" in df.columns else None)
    assert ycol is not None, f"CSV missing label column y(0..3)/y: {path}"
    need_cols = ["question","optA","optB","optC","optD"]
    for c in need_cols:
        assert c in df.columns, f"CSV missing column: {c}"

    def non_empty(s): return isinstance(s, str) and len(s.strip())>0
    rows=[]
    for _,r in df.iterrows():
        opts=[str(r["optA"]),str(r["optB"]),str(r["optC"]),str(r["optD"])]
        if not all(non_empty(o) for o in opts):
            continue
        y=int(r[ycol])
        if y not in (0,1,2,3): continue
        rows.append({"q": str(r["question"]), "opts": opts, "y": y})
    return rows

def load_items_from_jsonl(path):
    items=[]
    with open(path,"r",encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            q = obj.get("question","")
            opts = obj.get("options",[])
            y = obj.get("label", None)
            if not isinstance(opts,(list,tuple)) or len(opts)!=4:
                continue
            opts = [str(o) for o in opts]
            if y is None:
                al = obj.get("answer_letter")
                if isinstance(al,str) and al.upper() in "ABCD":
                    y="ABCD".index(al.upper())
            if y not in (0,1,2,3): continue
            items.append({"q":str(q), "opts":opts, "y":int(y)})
    return items

def load_subset(paths):
    last_err=None
    for p in paths:
        if not os.path.exists(p):
            continue
        try:
            if p.endswith('.csv'): it = load_items_from_csv(p)
            else: it = load_items_from_jsonl(p)
            if len(it)>0:
                print(f"[load] Loaded {len(it)} items from {p}")
                return it, p
            else:
                print(f"[load] {p} parsed 0 items")
        except Exception as e:
            last_err=e
            print(f"[load] Failed to read {p}: {e}")
    raise RuntimeError(f"No usable subset file found in {DATA_DIR}/. Error: {last_err}")

# PART 3: MODEL SCORER DEFINITIONS

class BaseScorer:
    """Abstract base class for a model scorer."""
    def __init__(self, model_name_or_path, device):
        self.model_name_or_path = model_name_or_path
        self.device = device
        self.tok = None
        self.model = None
        self.load_model()

    def load_model(self):
        """Loads the tokenizer and model."""
        raise NotImplementedError

    def score_items(self, items: List[dict]) -> (np.ndarray, np.ndarray):
        """
        Takes a list of items (dict with "q", "opts", "y")
        Returns (X_scores, y_all)
        - X_scores: numpy (N, 4) array of raw scores/logits
        - y_all: numpy (N,) array of integer labels
        """
        raise NotImplementedError

    @staticmethod
    def _norm_txt(s: str) -> str:
        """Helper for text normalization."""
        if s is None: return ""
        return " ".join(str(s).split())

# Scorer for DeBERTa-NLI

class DeBERTaNLIScorer(BaseScorer):
    """
    DeBERTa-NLI Scorer - Uses Natural Language Inference method
    """
    def __init__(self, model_name_or_path, device="cuda"):
        self.entailment_idx = None
        super().__init__(model_name_or_path, device)
        self._detect_entailment_idx()

    def load_model(self):
        print(f"\nLoading NLI model: {self.model_name_or_path}")
        self.tok = AutoTokenizer.from_pretrained(self.model_name_or_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name_or_path
        ).eval().to(self.device)
        print(f" Model loaded on {self.device}")

    def _detect_entailment_idx(self):
        """
        Automatically detect the class index corresponding to 'entailment'.
        """
        cfg = self.model.config
        idx = None
        label2id = getattr(cfg, "label2id", None)
        if isinstance(label2id, dict) and len(label2id) > 0:
            for name, i in label2id.items():
                if "entail" in str(name).lower():
                    idx = int(i)
                    break
        if idx is None:
            id2label = getattr(cfg, "id2label", None)
            if isinstance(id2label, dict) and len(id2label) > 0:
                for i, name in id2label.items():
                    if "entail" in str(name).lower():
                        idx = int(i)
                        break
        if idx is None:
            idx = 2
            print("Warning: Entailment label not found in config, using index 2 by default")
        else:
            print(f"Detected entailment index: {idx}")

        self.entailment_idx = idx

    @torch.no_grad()
    def score_option_nli(self, context: str, option: str) -> float:
        """
        Score a single option: Output: entailment logit (a scalar)
        """
        premise = self._norm_txt(context)
        hypothesis = self._norm_txt(option)

        inputs = self.tok(
            premise,
            hypothesis,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512,
        ).to(self.device)

        outputs = self.model(**inputs)
        logits = outputs.logits[0]  # shape: (num_labels,)
        entailment_logit = float(logits[self.entailment_idx].cpu().item())
        return entailment_logit

    @torch.no_grad()
    def score_items(self, items: List[dict]):
        X_scores, y_all = [], []
        print(f"\nScoring using {self.model_name_or_path} (NLI method)...")
        print(f"Using entailment index: {self.entailment_idx}")

        for ex in tqdm(items, desc="Scoring progress"):
            context = ex["q"]
            opts = ex["opts"]
            scores = []
            for opt in opts:
                score = self.score_option_nli(context, opt)
                scores.append(score)

            X_scores.append(scores)
            y_all.append(ex["y"])

        X_scores = np.array(X_scores, dtype=np.float64)
        y_all = np.array(y_all, dtype=int)

        preds = X_scores.argmax(axis=1)
        acc = (preds == y_all).mean()
        print(f"Quick validation - Raw accuracy: {acc:.4f}")

        return X_scores, y_all


# Scorer for Flan-T5 (Teacher-Forcing)

class T5Scorer(BaseScorer):
    def __init__(self, model_name_or_path="google/flan-t5-xl", device="cuda"):
        self.ANSWER_TEMPLATES = [
            "Answer: {opt}",
            "The correct answer is: {opt}",
            "Best ending: {opt}",
        ]
        self.LETTER_WEIGHT = 1.0
        self.FEWSHOT = """You are a careful and precise multiple-choice solver.
Pick the single best ending. Answer with only one letter among A, B, C, or D.

Example 1:
Context: The kid placed the book on the shelf and then he...
A) ate the shelf because it looked tasty.
B) put the books in the oven and turned it on.
C) adjusted the book so it stood upright with the others.
D) threw the book out of the window during a storm.
Answer: C

Example 2:
Context: To start a campfire, you would typically...
A) pour water on the wood to keep it wet.
B) gather dry tinder and use a spark or flame to ignite it.
C) bury the matches so animals won’t find them.
D) open the refrigerator to cool the wood first.
Answer: B

Example 3:
Context: The runner practiced every day so that she could...
A) forget how to run.
B) become slower and less coordinated.
C) improve her speed and endurance for the race.
D) sleep through the entire event.
Answer: C
---
"""
        super().__init__(model_name_or_path, device)

    def build_prompt(self, q, opts):
        return (self.FEWSHOT
                + f"Context: {q}\n"
                  f"A) {opts[0]}\nB) {opts[1]}\nC) {opts[2]}\nD) {opts[3]}\n"
                  f"Answer:")

    def load_model(self):
        print(f"\nLoading {self.model_name_or_path}...")
        self.tok = T5TokenizerFast.from_pretrained(self.model_name_or_path)
        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name_or_path).eval().to(self.device)

    @torch.no_grad()
    def seq_neglogp(self, prompt_ids, target_ids):
        labels = target_ids["input_ids"]
        if labels.dim() == 1:
            labels = labels.unsqueeze(0)  # [L] -> [1, L]

        out = self.model(
            input_ids=prompt_ids["input_ids"],
            attention_mask=prompt_ids["attention_mask"],
            labels=labels
        )
        avg_neglogp = float(out.loss.item())
        n_tok = int(labels.numel())
        return avg_neglogp * n_tok, n_tok

    @torch.no_grad()
    def option_text_logprob_avg(self, prompt: str, option_text: str, letter: str) -> float:
        enc = self.tok(prompt, return_tensors="pt", truncation=True, max_length=512).to(self.device)
        neglogps, ntoks = [], 0
        opt_str = self._norm_txt(option_text)
        for tpl in self.ANSWER_TEMPLATES:
            tgt_str = tpl.format(opt=opt_str, letter=letter)
            tgt = self.tok(tgt_str, return_tensors="pt", add_special_tokens=False).to(self.device)
            s, n = self.seq_neglogp(enc, tgt)
            neglogps.append(s); ntoks += n

        if ntoks == 0 or len(self.ANSWER_TEMPLATES) == 0:
            return -np.inf

        avg_neglogp_text = (sum(neglogps) / len(self.ANSWER_TEMPLATES)) / (ntoks / len(self.ANSWER_TEMPLATES))
        logp_text = -avg_neglogp_text

        if self.LETTER_WEIGHT > 0.0:
            tgt_letter = self.tok(letter, return_tensors="pt", add_special_tokens=False).to(self.device)
            sL, nL = self.seq_neglogp(enc, tgt_letter)
            if nL == 0:
                logp_letter = -np.inf
            else:
                avg_neglogp_letter = sL / nL
                logp_letter = -avg_neglogp_letter
            return (1.0 - self.LETTER_WEIGHT) * logp_text + self.LETTER_WEIGHT * logp_letter
        else:
            return logp_text

    def scores_ABCD(self, prompt: str, opts: List[str]):
        letters = ["A","B","C","D"]
        scores=[]
        for i in range(4):
            s = self.option_text_logprob_avg(prompt, opts[i], letters[i])
            scores.append(s)
        return np.array(scores, dtype=np.float64)

    def score_items(self, items: List[dict]) -> (np.ndarray, np.ndarray):
        X_scores, y_all = [], []
        for ex in tqdm(items, desc=f"Scoring with {self.model_name_or_path}"):
            prompt = self.build_prompt(ex["q"], ex["opts"])
            s = self.scores_ABCD(prompt, ex["opts"])
            X_scores.append(s); y_all.append(ex["y"])

        X_scores = np.vstack(X_scores).astype(np.float64)
        y_all = np.array(y_all, dtype=int)

        preds = X_scores.argmax(axis=1)
        acc = (preds == y_all).mean()
        print(f"Quick validation - Raw accuracy: {acc:.4f}")

        return X_scores, y_all

# -----------------
# Scorer for RoBERTa (Multiple-Choice Head)
# -----------------
class RobertaScorer(BaseScorer):
    def __init__(self, model_name_or_path="LIAMF-USP/roberta-large-finetuned-race", device="cuda"):
        super().__init__(model_name_or_path, device)

    def load_model(self):
        print(f"\nLoading {self.model_name_or_path}...")
        self.tok = RobertaTokenizerFast.from_pretrained(self.model_name_or_path)
        self.model = RobertaForMultipleChoice.from_pretrained(self.model_name_or_path).eval().to(self.device)

    @torch.no_grad()
    def score_items(self, items: List[dict]) -> (np.ndarray, np.ndarray):
        X_scores, y_all = [], []

        for ex in tqdm(items, desc=f"Scoring with {self.model_name_or_path}"):
            prompt = self._norm_txt(ex["q"])
            opts = [self._norm_txt(o) for o in ex["opts"]]

            inputs = self.tok(
                [prompt] * 4,  # Repeat the context 4 times
                opts,          # Pass the 4 options
                return_tensors="pt",
                padding="longest",
                truncation=True,
                max_length=512
            )

            inputs_with_batch = {key: value.unsqueeze(0).to(self.device) for key, value in inputs.items()}

            outputs = self.model(**inputs_with_batch)
            scores = outputs.logits.squeeze(0).cpu().numpy() # Shape (4,)

            X_scores.append(scores); y_all.append(ex["y"])

        X_scores = np.vstack(X_scores).astype(np.float64)
        y_all = np.array(y_all, dtype=int)

        preds = X_scores.argmax(axis=1)
        acc = (preds == y_all).mean()
        print(f"Quick validation - Raw accuracy: {acc:.4f}")

        return X_scores, y_all


# Scorer for Phi-3.5 (Letter-Only)

class PhiScorer(BaseScorer):
    def __init__(self, model_name_or_path, device="cuda"):
        self.FEWSHOT = """You are a careful and precise multiple-choice solver.
Pick the single best ending. Answer with only one letter among A, B, C, or D.

Example 1:
Context: The kid placed the book on the shelf and then he...
A) ate the shelf because it looked tasty.
B) put the books in the oven and turned it on.
C) adjusted the book so it stood upright with the others.
D) threw the book out of the window during a storm.
Answer: C

Example 2:
Context: To start a campfire, you would typically...
A) pour water on the wood to keep it wet.
B) gather dry tinder and use a spark or flame to ignite it.
C) bury the matches so animals won’t find them.
D) open the refrigerator to cool the wood first.
Answer: B

Example 3:
Context: The runner practiced every day so that she could...
A) forget how to run.
B) become slower and less coordinated.
C) improve her speed and endurance for the race.
D) sleep through the entire event.
Answer: C
---
"""
        super().__init__(model_name_or_path, device)

    def load_model(self):
        print(f"\nLoading model: {self.model_name_or_path}")
        self.tok = AutoTokenizer.from_pretrained(self.model_name_or_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name_or_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        ).eval()

        if self.tok.pad_token is None:
            self.tok.pad_token = self.tok.eos_token

        print(f" Model loaded on {self.device}")

    def build_prompt(self, q: str, opts: List[str]) -> str:
        """Construct few-shot + current question prompt"""
        return (
            self.FEWSHOT
            + f"Context: {q}\n"
            + f"A) {opts[0]}\nB) {opts[1]}\nC) {opts[2]}\nD) {opts[3]}\n"
            + "Answer:"
        )

    @torch.no_grad()
    def compute_log_likelihood(self, prompt_text: str, target_text: str) -> float:
        """
        Calculate log p(target_text | prompt_text) / len(target) (average log prob)
        """
        prompt_ids = self.tok(prompt_text, return_tensors="pt").input_ids.to(self.device)
        target_ids = self.tok(target_text, return_tensors="pt", add_special_tokens=False).input_ids.to(self.device)

        full_ids = torch.cat([prompt_ids, target_ids], dim=1)

        # Crucial: use_cache=False, to avoid past_key_values / DynamicCache related errors
        outputs = self.model(full_ids, use_cache=False)
        logits = outputs.logits  # [1, L, vocab]

        prompt_len = prompt_ids.shape[1]
        target_len = target_ids.shape[1]

        if target_len == 0:
            return -np.inf

        # Target starts from prompt_len, corresponding to logits from prompt_len-1
        target_logits = logits[0, prompt_len - 1 : prompt_len + target_len - 1, :]
        target_tokens = target_ids[0]

        log_probs = torch.log_softmax(target_logits, dim=-1)
        token_log_probs = log_probs[range(target_len), target_tokens]

        return float(token_log_probs.mean().item())

    @torch.no_grad()
    def letter_log_probs(self, prompt: str) -> np.ndarray:
        """
        Calculate log p(letter | prompt) for the 4 letters A/B/C/D as 4-dimensional logits
        """
        letters = ["A", "B", "C", "D"]
        scores = []
        for L in letters:
            lp = self.compute_log_likelihood(prompt, L)
            scores.append(lp)
        return np.array(scores, dtype=np.float64)

    @torch.no_grad()
    def score_items(self, items: List[dict]):
        print(f"\nScoring with {self.model_name_or_path} (letter-only)...")
        X_all, y_all = [], []

        for ex in tqdm(items, desc="Scoring"):
            prompt = self.build_prompt(ex["q"], ex["opts"])
            scores = self.letter_log_probs(prompt)  # (4,)
            X_all.append(scores)
            y_all.append(ex["y"])

        X_all = np.array(X_all, dtype=np.float64)  # (N,4)
        y_all = np.array(y_all, dtype=int)         # (N,)

        preds_raw = np.argmax(X_all, axis=1)
        acc_raw = (preds_raw == y_all).mean()
        print(f"\n Raw argmax accuracy (no TS): {acc_raw:.4f}")

        return X_all, y_all

# PART 4: CALIBRATION & METRIC UTILITIES

def robust_stratified_split(y_all, cal_ratio: float = 0.20, seed: int = SEED):
    """
    Stratified split data into calibration set (CAL) and test set (TEST).
    Takes cal_ratio as an argument.
    """
    y_all = np.asarray(y_all, dtype=int)
    max_ratio = 0.50
    r = cal_ratio
    best_split = None

    while r <= max_ratio:
        sss = StratifiedShuffleSplit(n_splits=1, test_size=1.0 - r, random_state=seed)
        cal_idx, test_idx = next(sss.split(np.zeros_like(y_all), y_all))

        cnt_cal = np.bincount(y_all[cal_idx], minlength=4)
        cnt_test = np.bincount(y_all[test_idx], minlength=4)

        if best_split is None:
             best_split = (np.sort(cal_idx), np.sort(test_idx), cnt_cal, cnt_test)

        if cnt_cal.min() > 0 and cnt_test.min() > 0:
            best_split = (np.sort(cal_idx), np.sort(test_idx), cnt_cal, cnt_test)
            break
        r += 0.05

    cal_idx, test_idx, cnt_cal, cnt_test = best_split

    print("Data Split:")
    print(f"  Calibration Set: {len(cal_idx)} samples, class distribution: {cnt_cal}")
    print(f"  Test Set: {len(test_idx)} samples, class distribution: {cnt_test}")
    if cnt_cal.min() == 0:
        print("[WARN] CAL set is missing a class; TS may be less reliable.")
    if cnt_test.min() == 0:
        print("[ERROR] TEST set is missing a class. Cannot evaluate reliably.")

    return cal_idx, test_idx

def softmax_row(v, T: float = 1.0):
    """
    Apply softmax to a single row of logits with temperature scaling.
    """
    v = np.asarray(v, dtype=np.float64) / max(T, 1e-6)

if __name__ == "__main__":

    os.makedirs(DATA_DIR, exist_ok=True)

    print("\n" + "="*80)
    print("CREATING HELLASWAG SUBSET DATASET")
    print("="*80)

    csv_path = os.path.join(DATA_DIR, "hellaswag_subset.csv")
    jsonl_path = os.path.join(DATA_DIR, "hellaswag_subset.jsonl")

    if os.path.exists(csv_path) and os.path.exists(jsonl_path):
        print(f"\nDataset files already exist:")
        print(f"  - {csv_path}")
        print(f"  - {jsonl_path}")
        print("\nSkipping creation. Delete these files if you want to regenerate.")
    else:
        print(f"\nGenerating new dataset in {DATA_DIR}/")
        create_hellaswag_subset()

        print("\n" + "="*80)
        print("DATASET CREATION COMPLETE")
        print("="*80)
        print(f"\nGenerated files:")
        print(f"  - {csv_path}")
        print(f"  - {jsonl_path}")

        items, _ = load_subset(DATA_PATHS)
        print(f"\nDataset contains {len(items)} samples")

"""# Part 2: Model Scorer Framework

This section defines the abstract BaseScorer class and implements four specific model scorers for different architectures.

## Contents:
- BaseScorer: Abstract base class defining the scorer interface
- DeBERTaNLIScorer: Natural Language Inference approach using DeBERTa
- T5Scorer: Teacher-forcing method with FLAN-T5
- RobertaScorer: Multiple-choice head approach with RoBERTa
- PhiScorer: Letter-only generation approach with Phi-3.5-mini-instruct

Each scorer implements the score_items() method to generate logits/scores for multiple-choice questions.
"""

class BaseScorer:
    """Abstract base class for a model scorer."""
    def __init__(self, model_name_or_path, device):
        self.model_name_or_path = model_name_or_path
        self.device = device
        self.tok = None
        self.model = None
        self.load_model()

    def load_model(self):
        """Loads the tokenizer and model."""
        raise NotImplementedError

    def score_items(self, items: List[dict]) -> (np.ndarray, np.ndarray):
        """
        Takes a list of items (dict with "q", "opts", "y")
        Returns (X_scores, y_all)
        - X_scores: numpy (N, 4) array of raw scores/logits
        - y_all: numpy (N,) array of integer labels
        """
        raise NotImplementedError

    @staticmethod
    def _norm_txt(s: str) -> str:
        """Helper for text normalization."""
        if s is None: return ""
        return " ".join(str(s).split())

class DeBERTaNLIScorer(BaseScorer):
    """
    DeBERTa-NLI Scorer - Uses Natural Language Inference method
    """
    def __init__(self, model_name_or_path, device="cuda"):
        self.entailment_idx = None
        super().__init__(model_name_or_path, device)
        self._detect_entailment_idx()

    def load_model(self):
        print(f"\nLoading NLI model: {self.model_name_or_path}")
        self.tok = AutoTokenizer.from_pretrained(self.model_name_or_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name_or_path
        ).eval().to(self.device)
        print(f" Model loaded on {self.device}")

    def _detect_entailment_idx(self):
        """
        Automatically detect the class index corresponding to 'entailment'.
        """
        cfg = self.model.config
        idx = None
        label2id = getattr(cfg, "label2id", None)
        if isinstance(label2id, dict) and len(label2id) > 0:
            for name, i in label2id.items():
                if "entail" in str(name).lower():
                    idx = int(i)
                    break
        if idx is None:
            id2label = getattr(cfg, "id2label", None)
            if isinstance(id2label, dict) and len(id2label) > 0:
                for i, name in id2label.items():
                    if "entail" in str(name).lower():
                        idx = int(i)
                        break
        if idx is None:
            idx = 2
            print("Warning: Entailment label not found in config, using index 2 by default")
        else:
            print(f"Detected entailment index: {idx}")

        self.entailment_idx = idx

    @torch.no_grad()
    def score_option_nli(self, context: str, option: str) -> float:
        """
        Score a single option: Output: entailment logit (a scalar)
        """
        premise = self._norm_txt(context)
        hypothesis = self._norm_txt(option)

        inputs = self.tok(
            premise,
            hypothesis,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512,
        ).to(self.device)

        outputs = self.model(**inputs)
        logits = outputs.logits[0]  # shape: (num_labels,)
        entailment_logit = float(logits[self.entailment_idx].cpu().item())
        return entailment_logit

    @torch.no_grad()
    def score_items(self, items: List[dict]):
        X_scores, y_all = [], []
        print(f"\nScoring using {self.model_name_or_path} (NLI method)...")
        print(f"Using entailment index: {self.entailment_idx}")

        for ex in tqdm(items, desc="Scoring progress"):
            context = ex["q"]
            opts = ex["opts"]
            scores = []
            for opt in opts:
                score = self.score_option_nli(context, opt)
                scores.append(score)

            X_scores.append(scores)
            y_all.append(ex["y"])

        X_scores = np.array(X_scores, dtype=np.float64)
        y_all = np.array(y_all, dtype=int)

        preds = X_scores.argmax(axis=1)
        acc = (preds == y_all).mean()
        print(f"Quick validation - Raw accuracy: {acc:.4f}")

        return X_scores, y_all

class T5Scorer(BaseScorer):
    def __init__(self, model_name_or_path="google/flan-t5-xl", device="cuda"):
        self.ANSWER_TEMPLATES = [
            "Answer: {opt}",
            "The correct answer is: {opt}",
            "Best ending: {opt}",
        ]
        self.LETTER_WEIGHT = 1.0
        self.FEWSHOT = """You are a careful and precise multiple-choice solver.
Pick the single best ending. Answer with only one letter among A, B, C, or D.

Example 1:
Context: The kid placed the book on the shelf and then he...
A) ate the shelf because it looked tasty.
B) put the books in the oven and turned it on.
C) adjusted the book so it stood upright with the others.
D) threw the book out of the window during a storm.
Answer: C

Example 2:
Context: To start a campfire, you would typically...
A) pour water on the wood to keep it wet.
B) gather dry tinder and use a spark or flame to ignite it.
C) bury the matches so animals won’t find them.
D) open the refrigerator to cool the wood first.
Answer: B

Example 3:
Context: The runner practiced every day so that she could...
A) forget how to run.
B) become slower and less coordinated.
C) improve her speed and endurance for the race.
D) sleep through the entire event.
Answer: C
---
"""
        super().__init__(model_name_or_path, device)

    def build_prompt(self, q, opts):
        return (self.FEWSHOT
                + f"Context: {q}\n"
                  f"A) {opts[0]}\nB) {opts[1]}\nC) {opts[2]}\nD) {opts[3]}\n"
                  f"Answer:")

    def load_model(self):
        print(f"\nLoading {self.model_name_or_path}...")
        self.tok = T5TokenizerFast.from_pretrained(self.model_name_or_path)
        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name_or_path).eval().to(self.device)

    @torch.no_grad()
    def seq_neglogp(self, prompt_ids, target_ids):
        labels = target_ids["input_ids"]
        if labels.dim() == 1:
            labels = labels.unsqueeze(0)  # [L] -> [1, L]

        out = self.model(
            input_ids=prompt_ids["input_ids"],
            attention_mask=prompt_ids["attention_mask"],
            labels=labels
        )
        avg_neglogp = float(out.loss.item())
        n_tok = int(labels.numel())
        return avg_neglogp * n_tok, n_tok

    @torch.no_grad()
    def option_text_logprob_avg(self, prompt: str, option_text: str, letter: str) -> float:
        enc = self.tok(prompt, return_tensors="pt", truncation=True, max_length=512).to(self.device)
        neglogps, ntoks = [], 0
        opt_str = self._norm_txt(option_text)
        for tpl in self.ANSWER_TEMPLATES:
            tgt_str = tpl.format(opt=opt_str, letter=letter)
            tgt = self.tok(tgt_str, return_tensors="pt", add_special_tokens=False).to(self.device)
            s, n = self.seq_neglogp(enc, tgt)
            neglogps.append(s); ntoks += n

        if ntoks == 0 or len(self.ANSWER_TEMPLATES) == 0:
            return -np.inf

        avg_neglogp_text = (sum(neglogps) / len(self.ANSWER_TEMPLATES)) / (ntoks / len(self.ANSWER_TEMPLATES))
        logp_text = -avg_neglogp_text

        if self.LETTER_WEIGHT > 0.0:
            tgt_letter = self.tok(letter, return_tensors="pt", add_special_tokens=False).to(self.device)
            sL, nL = self.seq_neglogp(enc, tgt_letter)
            if nL == 0:
                logp_letter = -np.inf
            else:
                avg_neglogp_letter = sL / nL
                logp_letter = -avg_neglogp_letter
            return (1.0 - self.LETTER_WEIGHT) * logp_text + self.LETTER_WEIGHT * logp_letter
        else:
            return logp_text

    def scores_ABCD(self, prompt: str, opts: List[str]):
        letters = ["A","B","C","D"]
        scores=[]
        for i in range(4):
            s = self.option_text_logprob_avg(prompt, opts[i], letters[i])
            scores.append(s)
        return np.array(scores, dtype=np.float64)

    def score_items(self, items: List[dict]) -> (np.ndarray, np.ndarray):
        X_scores, y_all = [], []
        for ex in tqdm(items, desc=f"Scoring with {self.model_name_or_path}"):
            prompt = self.build_prompt(ex["q"], ex["opts"])
            s = self.scores_ABCD(prompt, ex["opts"])
            X_scores.append(s); y_all.append(ex["y"])

        X_scores = np.vstack(X_scores).astype(np.float64)
        y_all = np.array(y_all, dtype=int)

        preds = X_scores.argmax(axis=1)
        acc = (preds == y_all).mean()
        print(f"Quick validation - Raw accuracy: {acc:.4f}")

        return X_scores, y_all

class RobertaScorer(BaseScorer):
    def __init__(self, model_name_or_path="LIAMF-USP/roberta-large-finetuned-race", device="cuda"):
        super().__init__(model_name_or_path, device)

    def load_model(self):
        print(f"\nLoading {self.model_name_or_path}...")
        self.tok = RobertaTokenizerFast.from_pretrained(self.model_name_or_path)
        self.model = RobertaForMultipleChoice.from_pretrained(self.model_name_or_path).eval().to(self.device)

    @torch.no_grad()
    def score_items(self, items: List[dict]) -> (np.ndarray, np.ndarray):
        X_scores, y_all = [], []

        for ex in tqdm(items, desc=f"Scoring with {self.model_name_or_path}"):
            prompt = self._norm_txt(ex["q"])
            opts = [self._norm_txt(o) for o in ex["opts"]]

            inputs = self.tok(
                [prompt] * 4,  # Repeat the context 4 times
                opts,          # Pass the 4 options
                return_tensors="pt",
                padding="longest",
                truncation=True,
                max_length=512
            )

            inputs_with_batch = {key: value.unsqueeze(0).to(self.device) for key, value in inputs.items()}

            outputs = self.model(**inputs_with_batch)
            scores = outputs.logits.squeeze(0).cpu().numpy() # Shape (4,)

            X_scores.append(scores); y_all.append(ex["y"])

        X_scores = np.vstack(X_scores).astype(np.float64)
        y_all = np.array(y_all, dtype=int)

        preds = X_scores.argmax(axis=1)
        acc = (preds == y_all).mean()
        print(f"Quick validation - Raw accuracy: {acc:.4f}")

        return X_scores, y_all

class PhiScorer(BaseScorer):
    def __init__(self, model_name_or_path, device="cuda"):
        self.FEWSHOT = """You are a careful and precise multiple-choice solver.
Pick the single best ending. Answer with only one letter among A, B, C, or D.

Example 1:
Context: The kid placed the book on the shelf and then he...
A) ate the shelf because it looked tasty.
B) put the books in the oven and turned it on.
C) adjusted the book so it stood upright with the others.
D) threw the book out of the window during a storm.
Answer: C

Example 2:
Context: To start a campfire, you would typically...
A) pour water on the wood to keep it wet.
B) gather dry tinder and use a spark or flame to ignite it.
C) bury the matches so animals won’t find them.
D) open the refrigerator to cool the wood first.
Answer: B

Example 3:
Context: The runner practiced every day so that she could...
A) forget how to run.
B) become slower and less coordinated.
C) improve her speed and endurance for the race.
D) sleep through the entire event.
Answer: C
---
"""
        super().__init__(model_name_or_path, device)

    def load_model(self):
        print(f"\nLoading model: {self.model_name_or_path}")
        self.tok = AutoTokenizer.from_pretrained(self.model_name_or_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name_or_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        ).eval()

        if self.tok.pad_token is None:
            self.tok.pad_token = self.tok.eos_token

        print(f" Model loaded on {self.device}")

    def build_prompt(self, q: str, opts: List[str]) -> str:
        """Construct few-shot + current question prompt"""
        return (
            self.FEWSHOT
            + f"Context: {q}\n"
            + f"A) {opts[0]}\nB) {opts[1]}\nC) {opts[2]}\nD) {opts[3]}\n"
            + "Answer:"
        )

    @torch.no_grad()
    def compute_log_likelihood(self, prompt_text: str, target_text: str) -> float:
        """
        Calculate log p(target_text | prompt_text) / len(target) (average log prob)
        """
        prompt_ids = self.tok(prompt_text, return_tensors="pt").input_ids.to(self.device)
        target_ids = self.tok(target_text, return_tensors="pt", add_special_tokens=False).input_ids.to(self.device)

        full_ids = torch.cat([prompt_ids, target_ids], dim=1)

        # Crucial: use_cache=False, to avoid past_key_values / DynamicCache related errors
        outputs = self.model(full_ids, use_cache=False)
        logits = outputs.logits  # [1, L, vocab]

        prompt_len = prompt_ids.shape[1]
        target_len = target_ids.shape[1]

        if target_len == 0:
            return -np.inf

        # Target starts from prompt_len, corresponding to logits from prompt_len-1
        target_logits = logits[0, prompt_len - 1 : prompt_len + target_len - 1, :]
        target_tokens = target_ids[0]

        log_probs = torch.log_softmax(target_logits, dim=-1)
        token_log_probs = log_probs[range(target_len), target_tokens]

        return float(token_log_probs.mean().item())

    @torch.no_grad()
    def letter_log_probs(self, prompt: str) -> np.ndarray:
        """
        Calculate log p(letter | prompt) for the 4 letters A/B/C/D as 4-dimensional logits
        """
        letters = ["A", "B", "C", "D"]
        scores = []
        for L in letters:
            lp = self.compute_log_likelihood(prompt, L)
            scores.append(lp)
        return np.array(scores, dtype=np.float64)

    @torch.no_grad()
    def score_items(self, items: List[dict]):
        print(f"\nScoring with {self.model_name_or_path} (letter-only)...")
        X_all, y_all = [], []

        for ex in tqdm(items, desc="Scoring"):
            prompt = self.build_prompt(ex["q"], ex["opts"])
            scores = self.letter_log_probs(prompt)  # (4,)
            X_all.append(scores)
            y_all.append(ex["y"])

        X_all = np.array(X_all, dtype=np.float64)  # (N,4)
        y_all = np.array(y_all, dtype=int)         # (N,)

        preds_raw = np.argmax(X_all, axis=1)
        acc_raw = (preds_raw == y_all).mean()
        print(f"\n Raw argmax accuracy (no TS): {acc_raw:.4f}")

        return X_all, y_all

"""# Part 3: Temperature Scaling and Calibration Metrics

This section implements temperature scaling for probability calibration and defines metrics to evaluate calibration quality.

## Contents:
- softmax_row: Apply temperature-scaled softmax to logits
- renorm_rows: Normalize probability matrices
- fit_temperature_grid: Grid search to find optimal temperature parameter
- ece_multiclass: Expected Calibration Error metric
- brier_multiclass: Brier score metric
- calib_metrics: Comprehensive calibration evaluation function

Temperature scaling improves model calibration by scaling logits before softmax, optimized to minimize negative log-likelihood on a calibration set.
"""

def softmax_row(v, T: float = 1.0):
    """
    Apply softmax to a single row of logits with temperature scaling.
    """
    v = np.asarray(v, dtype=np.float64) / max(T, 1e-6)
    v = v - np.max(v)  # Numerical stability
    expv = np.exp(np.clip(v, -50, 50))
    p = expv / np.sum(expv)
    p = np.clip(p, 1e-8, 1 - 1e-8)
    return p / p.sum()

def renorm_rows(P):
    """
    Clean up numerical values and re-normalize rows of a probability matrix
    """
    P = np.nan_to_num(P, nan=1e-8, posinf=1e8, neginf=1e-8)
    P = np.clip(P, 1e-8, 1 - 1e-8)
    return P / P.sum(axis=1, keepdims=True)

def fit_temperature_grid(S, y_true, T_min: float = 0.05, T_max: float = 50.0, num: int = 80) -> float:
    """
    Grid search for the temperature T that minimizes NLL.
    """
    Ts = np.exp(np.linspace(np.log(T_min), np.log(T_max), num))
    best_T, best_nll = 1.0, np.inf

    print(f"\nTemperature Scaling: Searching range [{T_min:.2f}, {T_max:.2f}]")
    for T in Ts:
        P = renorm_rows(np.vstack([softmax_row(s, T) for s in S]))
        nll = log_loss(y_true, P, labels=[0, 1, 2, 3])
        if nll < best_nll:
            best_T, best_nll = T, nll

    print(f"Optimal temperature: T = {best_T:.3f}, NLL = {best_nll:.4f}")
    return float(best_T)

def ece_multiclass(P: np.ndarray, y: np.ndarray, bins: int = 15) -> float:
    """Expected Calibration Error (ECE)"""
    conf = P.max(1)
    preds = P.argmax(1)
    corr = (preds == y).astype(float)
    edges = np.linspace(0, 1, bins + 1)
    ece = 0.0
    total_n = len(P)

    for i in range(bins):
        m = (conf >= edges[i]) & (conf < edges[i + 1])
        n_in_bin = m.sum()
        if n_in_bin == 0:
            continue

        bin_acc = corr[m].mean()
        bin_conf = conf[m].mean()
        ece += (n_in_bin / total_n) * abs(bin_acc - bin_conf)

    return float(ece)

def brier_multiclass(P: np.ndarray, y: np.ndarray) -> float:
    """Brier Score"""
    Y = np.eye(P.shape[1])[y]
    return float(np.mean(np.sum((P - Y) ** 2, axis=1)))

def calib_metrics(P: np.ndarray, y: np.ndarray, name: str, bins: int = 15):
    """Calculate all calibration metrics and print them"""
    acc = float((P.argmax(1) == y).mean())
    nll = float(log_loss(y, P, labels=[0, 1, 2, 3]))
    ece = float(ece_multiclass(P, y, bins=bins))
    bri = float(brier_multiclass(P, y))

    print(f"\n[{name}]")
    print(f"  Accuracy:     {acc:.4f}")
    print(f"  NLL:          {nll:.4f}")
    print(f"  ECE ({bins} bins):  {ece:.4f}")
    print(f"  Brier Score:  {bri:.4f}")

    return {
        "Method": name,
        "Accuracy": acc,
        "NLL": nll,
        "ECE": ece,
        "Brier": bri,
    }

"""# Part 4: Risk-Coverage Functions

This section implements selective prediction (abstention) analysis using risk-coverage curves and related metrics.

## Contents:
- confidences: Extract maximum confidence from probability matrix
- rc_from_P: Compute risk-coverage curve from predictions
- aurc: Calculate Area Under Risk-Coverage curve
- fit_r_hat: Fit isotonic regression model to estimate risk at confidence thresholds
- choose_tau_by_iso: Select optimal confidence threshold for target risk level
- C_at_R: Compute coverage at a given risk level
- R_at_C: Compute risk at a given coverage level
- binned_acc: Calculate binned accuracy for confidence intervals
- acc_minus_conf: Calculate accuracy minus confidence gaps across bins

These functions enable risk-aware calibration where models can abstain from predictions when confidence is below a threshold, achieving desired risk-coverage tradeoffs.
"""

def confidences(P: np.ndarray):
    """Extract the maximum confidence max_k p_k for each sample"""
    return P.max(1)

def rc_from_P(P: np.ndarray, y: np.ndarray, bins: int = 600):
    """
    Calculate Risk–Coverage curve.
    """
    conf = confidences(P)
    pred = P.argmax(1)
    correct = (pred == y).astype(float)

    # Sort by confidence
    idx_sorted = np.argsort(-conf) # Descending confidence
    conf_sorted = conf[idx_sorted]
    correct_sorted = correct[idx_sorted]

    n = len(P)
    cov, risk = [], []

    # Calculate risk at different coverage levels
    for k in range(1, n + 1):
        # Coverage is k/n
        current_cov = k / n
        # Risk is error rate of the top-k most confident items
        current_correct = correct_sorted[:k].sum()
        current_risk = 1.0 - (current_correct / k)

        cov.append(current_cov)
        risk.append(current_risk)

    # Add (0,0) point
    cov = np.array([0] + cov)
    risk = np.array([0] + risk) # Risk at 0 coverage is 0

    return cov, risk

def aurc(cov: np.ndarray, risk: np.ndarray) -> float:
    """Area Under Risk–Coverage curve (AURC)"""
    # Ensure coverage is sorted for trapezoidal rule
    order = np.argsort(cov)
    return float(np.trapz(risk[order], cov[order]))

def fit_r_hat(P_cal: np.ndarray, y_cal: np.ndarray):
    """
    Fit a monotonic regression of "confidence -> predicted risk" on the CAL set.
    """
    conf = confidences(P_cal)
    risk01 = (P_cal.argmax(1) != y_cal).astype(float)
    ir = IsotonicRegression(
        y_min=0.0,
        y_max=1.0,
        increasing=True, # Risk increases as -confidence increases
        out_of_bounds="clip",
    )
    # Use -conf, so that "higher confidence" -> "lower risk"
    # We fit (-conf) -> (risk01), which is an increasing relationship
    ir.fit(-conf, risk01)
    # Predict takes -c, so it maps high confidence (low -c) to low risk
    return lambda c: ir.predict(-np.asarray(c))

def choose_tau_by_iso(
    P_cal: np.ndarray,
    y_cal: np.ndarray,
    r_target: float,
    min_cov: float = 0.05,
    min_n: int = 30,
):
    """
    Select confidence threshold tau on CAL set that achieves r_target.
    """
    conf = confidences(P_cal)
    r_hat_func = fit_r_hat(P_cal, y_cal)

    grid = np.linspace(conf.min(), conf.max(), 5000)
    best = None

    for t in grid:
        m = (conf >= t)
        cov = m.mean()
        n = int(m.sum())
        if cov < min_cov or n < min_n:
            continue

        # Estimated risk at this threshold
        est_risk = float(r_hat_func([t])[0])

        gap = max(0.0, est_risk - r_target)

        # Primary sort key: penalty (gap)
        # Secondary sort key: maximize coverage (-cov)
        key = (gap, -cov)

        if best is None or key < best[0]:
            best = (key, t)

    return None if best is None else float(best[1])

def C_at_R(cov, risk, r_star):
    """Given a target risk r_star, return the coverage closest to that risk point"""
    # Find all points *at or below* the target risk
    valid_points = np.where(risk <= r_star)[0]
    if len(valid_points) == 0:
        # If no point is below target risk, return 0 coverage
        return 0.0, risk.min()
    # Of those points, find the one with maximum coverage
    k = valid_points[np.argmax(cov[valid_points])]
    return float(cov[k]), float(risk[k])

def R_at_C(cov, risk, c_star):
    """Given a target coverage c_star, return the risk closest to that coverage point"""
    # Find the index where coverage is closest to c_star
    k = int(np.argmin(np.abs(cov - c_star)))
    return float(risk[k]), float(cov[k])

def binned_acc(P: np.ndarray, y: np.ndarray, edges: np.ndarray):
    """Helper: Calculate accuracy in bins defined by edges"""
    conf = P.max(1)
    preds = P.argmax(1)
    corr = (preds == y).astype(float)
    centers = (edges[:-1] + edges[1:]) / 2
    accs = []
    for i in range(len(edges) - 1):
        m = (conf >= edges[i]) & (conf < edges[i + 1])
        if m.sum() == 0:
            accs.append(centers[i])
        else:
            accs.append(corr[m].mean())
    return np.array(accs)

def acc_minus_conf(P: np.ndarray, y: np.ndarray, edges: np.ndarray):
    """Helper: Calculate (Accuracy - Confidence) in bins"""
    conf = P.max(1)
    preds = P.argmax(1)
    corr = (preds == y).astype(float)
    deltas = []
    for i in range(len(edges) - 1):
        m = (conf >= edges[i]) & (conf < edges[i + 1])
        if m.sum() == 0:
            deltas.append(0.0)
        else:
            bin_acc = corr[m].mean()
            bin_conf = conf[m].mean()
            deltas.append(bin_acc - bin_conf)
    return np.array(deltas)

"""# Part 5: Visualization Functions

This section implements plotting functions for visualizing calibration quality and risk-coverage analysis.

## Contents:
- equal_mass_bins: Helper function to compute binned statistics with Wilson score confidence intervals
- plot_rc_curve: Plot Risk-Coverage curves comparing pre/post temperature scaling
- plot_calibration_scatter: Scatter plot showing confidence vs accuracy with binning
- plot_calib_residual: Residual plot of (accuracy - confidence) across bins
- plot_calib_hist: Histogram of confidence distribution with correctness overlay
- plot_car_bars: Bar chart comparing Coverage at Risk metrics across methods

All visualization functions save outputs as PNG files to the specified model output directory.
"""

# =============================================================================
# VISUALIZATION FUNCTIONS
# =============================================================================

def equal_mass_bins(conf: np.ndarray, corr: np.ndarray, nbins: int = 10):
    """Calculate binned accuracy in equal-mass bins with Wilson score confidence intervals"""
    n = len(conf)
    indices = np.argsort(conf)
    conf_sorted = conf[indices]
    corr_sorted = corr[indices]

    bin_size = n // nbins
    bin_edges = [i * bin_size for i in range(nbins)] + [n]

    xs, acc, ns, lo, hi = [], [], [], [], []

    for i in range(nbins):
        start = bin_edges[i]
        end = bin_edges[i+1]
        if start == end:
            continue

        bin_conf = conf_sorted[start:end]
        bin_corr = corr_sorted[start:end]

        if len(bin_conf) == 0:
            continue

        xs.append(bin_conf.mean())
        acc.append(bin_corr.mean())
        ns.append(len(bin_corr))

        p_hat = bin_corr.mean()
        n_bin = len(bin_corr)
        z = 1.645  # 90% confidence interval

        if n_bin == 0:
            lo.append(0.0)
            hi.append(1.0)
            continue

        A = p_hat + (z**2) / (2 * n_bin)
        B = z * np.sqrt((p_hat * (1 - p_hat) / n_bin) + (z**2 / (4 * n_bin**2)))
        C = 1 + (z**2 / n_bin)

        lo_val = max(0.0, (A - B) / C)
        hi_val = min(1.0, (A + B) / C)

        lo.append(lo_val)
        hi.append(hi_val)

    return np.array(xs), np.array(acc), np.array(ns), np.array(lo), np.array(hi)


def plot_calibration_scatter(P_dict, y_test, model_name_tag: str, model_outdir: str):
    """
    Plot calibration scatter with density hexbins, equal-mass bins with CI, and linear fit
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    for idx, (method_name, P) in enumerate(P_dict.items()):
        ax = axes[idx]
        conf = confidences(P)
        corr = (P.argmax(1) == y_test).astype(float)
        color = "gray" if method_name == "NoTS" else "blue"
        cmap = "Greys" if method_name == "NoTS" else "Blues"

        # Hexbin density visualization
        ax.hexbin(
            conf,
            corr,
            gridsize=40,
            extent=(0, 1, -0.05, 1.05),
            cmap=cmap,
            mincnt=1,
            bins="log",
            alpha=0.6,
        )

        # Perfect calibration line
        ax.plot([0, 1], [0, 1], "k--", linewidth=1.5, label="Perfect Calibration")

        # Equal-mass bins with error bars
        xs, acc, n, lo, hi = equal_mass_bins(conf, corr, nbins=10)
        yerr_lower = np.maximum(0.0, acc - lo)
        yerr_upper = np.maximum(0.0, hi - acc)

        # Linear fit on binned data
        if len(xs) > 1:
            coeffs = np.polyfit(xs, acc, 1)
            slope = coeffs[0]
            fit_line = np.poly1d(coeffs)
            x_fit = np.linspace(0, 1, 100)
            ax.plot(x_fit, fit_line(x_fit), color=color, linewidth=2,
                   label=f"linear fit (slope={slope:.2f})")

        ax.errorbar(xs, acc, yerr=[yerr_lower, yerr_upper],
                   fmt="o", color=color, capsize=3, markersize=6,
                   label="equal-mass bins")

        ax.set_xlim(0, 1)
        ax.set_ylim(-0.05, 1.05)
        ax.set_xlabel("Confidence", fontsize=11)
        ax.set_ylabel("Correct (0/1)", fontsize=11)
        ax.set_title(f"Scatter + Density + CI — {method_name}", fontsize=12)
        ax.legend(loc="upper left", fontsize=9)
        ax.grid(alpha=0.3)

    plt.suptitle(f"Calibration Analysis - {model_name_tag}", fontsize=14, y=1.02)
    out_path = os.path.join(model_outdir, f"calibration_scatter_{model_name_tag}.png")
    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.show()
    print(f"Saved plot: {out_path}")


def plot_calib_hist(P_dict, y_test, model_name_tag: str, model_outdir: str):
    """Plot reliability diagram with confidence distribution overlay"""
    edges = np.linspace(0, 1, 11)
    centers = (edges[:-1] + edges[1:]) / 2
    conf_TS = confidences(P_dict["TS"])
    conf_NoT = confidences(P_dict["NoTS"])

    fig, ax1 = plt.subplots(figsize=(7, 4))
    ax1.plot(centers, binned_acc(P_dict["NoTS"], y_test, edges),
            "o-", color="gray", label="NoTS")
    ax1.plot(centers, binned_acc(P_dict["TS"], y_test, edges),
            "o-", color="blue", label="TS")
    ax1.plot([0, 1], [0, 1], "k--", lw=1)
    ax1.set_xlabel("Confidence")
    ax1.set_ylabel("Accuracy")
    ax1.set_ylim(-0.05, 1.05)

    ax2 = ax1.twinx()
    ax2.hist(conf_NoT, bins=edges, color="gray", alpha=0.25,
            weights=np.ones_like(conf_NoT)/len(conf_NoT))
    ax2.hist(conf_TS, bins=edges, color="blue", alpha=0.25,
            weights=np.ones_like(conf_TS)/len(conf_TS))
    ax2.set_ylabel("Fraction of Samples")

    ax1.legend(loc="lower right")
    ax1.set_title(f"Reliability + Conf Dist - {model_name_tag}")

    out_fig = os.path.join(model_outdir, f"fig_calib_hist_{model_name_tag}.png")
    plt.tight_layout()
    plt.savefig(out_fig, dpi=160)
    plt.show()
    print(f"Saved figure: {out_fig}")

"""# Part 6: Main Pipeline Function

This section contains the orchestration function that executes the complete risk-aware calibration pipeline.

## Contents:
- run_full_pipeline: Main execution function that coordinates all pipeline steps
- summary_row: Helper function to generate summary statistics for abstention analysis

## Pipeline Steps:
1. Data preparation: Load or create HellaSwag subset
2. Model scoring: Generate predictions or load from cache
3. Data splitting: Stratified split into calibration and test sets
4. Temperature scaling: Optimize temperature parameter on calibration set
5. Calibration metrics: Compute ECE, Brier score, accuracy, NLL
6. Risk-coverage analysis: Dynamic risk targets based on model performance
7. Abstention analysis: Select thresholds and evaluate coverage at risk
8. Visualization: Generate all diagnostic plots
9. Results saving: Export metrics and summaries to CSV files

The function accepts a scorer configuration dictionary and calibration ratio as primary arguments.
"""

def run_full_pipeline(
    scorer_config: dict,
    cal_ratio: float, # <-- NEW ARGUMENT
    force_recreate_data: bool = False,
    force_rescore: bool = False,
    model_outdir: str = "runs" # Default, though should be overridden
):
    """
    Run the complete risk-aware calibration pipeline.
    """
    print("\n" + "=" * 60)
    print(f"Running Pipeline for: {scorer_config['tag']}")
    print(f"Calibration Ratio: {cal_ratio:.2f} (CAL samples: {int(cal_ratio * MAX_N_LOAD)})")
    print(f"Saving results to: {model_outdir}")
    print("=" * 60)

    model_tag = scorer_config['tag']

    jsonl_path = os.path.join(DATA_DIR, "hellaswag_subset.jsonl")
    if force_recreate_data or not os.path.exists(jsonl_path):
        create_hellaswag_subset()
    else:
        print(f"\nData file already exists, skipping creation step")

    items, _ = load_subset(DATA_PATHS)
    items = items[:min(MAX_N_LOAD, len(items))]

    cache_scores_path = os.path.join(model_outdir, f"cache_scores_{model_tag}.npz")

    # NOTE: To prevent massive downloads on every ratio change, we check for a
    # model-wide score cache first. This cache is saved by the FIRST ratio run.
    model_base_dir = os.path.dirname(os.path.dirname(model_outdir)) # RESULTS_DIR/model_tag/
    model_wide_cache_path = os.path.join(model_base_dir, f"cache_scores_{model_tag}_FULL.npz")

    X_scores, y_all = None, None

    if os.path.exists(model_wide_cache_path) and not force_rescore:
        print(f"\nLoading scores from model-wide cache: {model_wide_cache_path}")
        data = np.load(model_wide_cache_path)
        X_scores, y_all = data["X_scores"], data["y_all"]
    elif force_rescore or not os.path.exists(model_wide_cache_path):
        # Run scoring if no cache exists or forced
        print(f"\nCache file not found or force_rescore=True. Running scoring...")

        if scorer_config['type'] == 'deberta_nli':
            scorer = DeBERTaNLIScorer(scorer_config['name_or_path'], device=device)
        elif scorer_config['type'] == 't5':
            scorer = T5Scorer(scorer_config['name_or_path'], device=device)
        elif scorer_config['type'] == 'roberta':
            scorer = RobertaScorer(scorer_config['name_or_path'], device=device)
        elif scorer_config['type'] == 'phi':
            scorer = PhiScorer(scorer_config['name_or_path'], device=device)
        else:
            raise ValueError(f"Unknown scorer type: {scorer_config['type']}")

        X_scores, y_all = scorer.score_items(items)
        np.savez(model_wide_cache_path, X_scores=X_scores, y_all=y_all)
        print(f"Model-wide scoring cache saved: {model_wide_cache_path}")

    assert X_scores is not None and y_all is not None, "Failed to load or generate scores."

    print(f"\nDataset size: {len(X_scores)} samples")
    print(f"Logits shape: {X_scores.shape}")

    cal_idx, test_idx = robust_stratified_split(y_all, cal_ratio=cal_ratio, seed=SEED) # <-- USES NEW ARGUMENT
    S_cal, y_cal = X_scores[cal_idx], y_all[cal_idx]
    S_test, y_test = X_scores[test_idx], y_all[test_idx]

    print("\n" + "=" * 60)
    print("Temperature Scaling")
    print("=" * 60)

    T_opt = fit_temperature_grid(S_cal, y_cal)

    P_cal_noTS = renorm_rows(np.vstack([softmax_row(s, T=1.0) for s in S_cal]))
    P_test_noTS = renorm_rows(np.vstack([softmax_row(s, T=1.0) for s in S_test]))
    P_cal_TS = renorm_rows(np.vstack([softmax_row(s, T=T_opt) for s in S_cal]))
    P_test_TS = renorm_rows(np.vstack([softmax_row(s, T=T_opt) for s in S_test]))

    # Save unified TS cache
    cache_ts_path = os.path.join(model_outdir, f"cache_ts_{model_tag}.npz")
    np.savez(
        cache_ts_path,
        p_noT_cal=P_cal_noTS, p_noT_test=P_test_noTS,
        p_T_cal=P_cal_TS, p_T_test=P_test_TS,
        y_cal=y_cal, y_test=y_test,
        T_star=T_opt,
    )
    print(f"Temperature Scaling cache saved: {cache_ts_path}")

    print("\n" + "=" * 60)
    print("Calibration Metrics Evaluation (TEST)")
    print("=" * 60)

    metrics = []
    metrics.append(calib_metrics(P_test_noTS, y_test, "NoTS", bins=15))
    metrics.append(calib_metrics(P_test_TS,  y_test, "TS", bins=15))

    df_metrics = pd.DataFrame(metrics)
    metrics_path = os.path.join(model_outdir, f"summary_calibration_{model_tag}.csv")
    df_metrics.to_csv(metrics_path, index=False)
    print(f"Metrics saved: {metrics_path}")
    print(df_metrics.to_string(index=False))

    print("\n" + "=" * 60)
    print("Abstention Threshold Analysis (based on CAL, post-TS probabilities)")
    print("=" * 60)

    # Define risk targets based on the model's performance
    base_risk = 1.0 - df_metrics[df_metrics['Method']=='TS']['Accuracy'].values[0]
    # Example: target risks from 10% better than baseline up to 10%
    risk_targets = np.linspace(max(0.05, base_risk - 0.10), min(0.9, base_risk + 0.10), 5)
    print(f"Base risk is {base_risk:.3f}, setting dynamic risk targets: {[f'{r:.2f}' for r in risk_targets]}")

    print("\nAbstention thresholds at target risk levels (using isotonic regression):")
    tau_rows = []
    for r_target in risk_targets:
        tau = choose_tau_by_iso(P_cal_TS, y_cal, r_target=r_target, min_cov=0.05, min_n=20)
        if tau is None:
            print(f"  r_target={r_target:.2f}: No tau found satisfying conditions")
            tau_rows.append({"R_target": r_target, "note": "infeasible on CAL"})
            continue

        conf_cal = confidences(P_cal_TS); m_cal = (conf_cal >= tau)
        cov_cal = m_cal.mean()
        risk_cal = 0.0 if not m_cal.any() else 1.0 - (P_cal_TS.argmax(1)[m_cal] == y_cal[m_cal]).mean()
        conf_test = confidences(P_test_TS); m_test = (conf_test >= tau)
        cov_test = m_test.mean()
        risk_test = 0.0 if not m_test.any() else 1.0 - (P_test_TS.argmax(1)[m_test] == y_test[m_test]).mean()

        print(
            f"  r_target={r_target:.2f}: "
            f"tau={tau:.4f}, "
            f"[CAL]  Cov={cov_cal:.3f}, Risk={risk_cal:.3f}, n={int(m_cal.sum())}  |  "
            f"[TEST] Cov={cov_test:.3f}, Risk={risk_test:.3f}, n={int(m_test.sum())}"
        )

        tau_rows.append(dict(R_target=r_target, tau=tau,
                             cov_cal=cov_cal, risk_cal=risk_cal, n_cal=int(m_cal.sum()),
                             cov_test=cov_test, risk_test=risk_test, n_test=int(m_test.sum())))

    df_tau = pd.DataFrame(tau_rows)
    tau_path = os.path.join(model_outdir, f"summary_taus_isotonic_{model_tag}.csv")
    df_tau.to_csv(tau_path, index=False)
    print("Saved tau summary:", tau_path)

    # Step 6b: R@C / C@R Summary Table
    print("\n" + "=" * 60)
    print("R@C / C@R Summary (TEST)")
    print("=" * 60)

    cov_NoT, risk_NoT = rc_from_P(P_test_noTS, y_test)
    cov_TS,  risk_TS  = rc_from_P(P_test_TS,  y_test)

    # Define coverage targets
    coverage_targets = [0.20, 0.30, 0.40, 0.60, 0.80]

    def summary_row(tag, cov_curve, risk_curve):
        row = {"Method": tag, "AURC": aurc(cov_curve, risk_curve)}
        for c in coverage_targets:
            r, _ = R_at_C(cov_curve, risk_curve, c)
            row[f"R@C{int(c*100)}"] = r
        for r in risk_targets:
            c, _ = C_at_R(cov_curve, risk_curve, r)
            row[f"C@R{int(r*100)}"] = c
        return row

    df_summary = pd.DataFrame([
        summary_row("NoT", cov_NoT, risk_NoT),
        summary_row("TS",  cov_TS,  risk_TS),
    ])

    print("\n=== Summary: R@C / C@R (TEST) ===")
    print(df_summary.to_string(index=False))
    summary_path = os.path.join(model_outdir, f"summary_abstain_{model_tag}.csv")
    df_summary.to_csv(summary_path, index=False)
    print("Saved:", summary_path)

    print("\n" + "=" * 60)
    print("Generating Visualizations")
    print("=" * 60)

    P_dict_test = {"NoTS": P_test_noTS, "TS": P_test_TS}

    plot_calibration_scatter(P_dict_test, y_test, model_tag, model_outdir)
    plot_calib_hist(P_dict_test, y_test, model_tag, model_outdir)

    print("\n" + "=" * 60)
    print(f"Pipeline Complete for {model_tag}.")
    print("=" * 60)
    print(f"\nAll outputs saved in: {model_outdir}/")

    return df_metrics, df_summary

"""# RoBERTa Model Execution

This section executes the complete risk-aware calibration pipeline for the RoBERTa model.

## Model Configuration:
- Model: LIAMF-USP/roberta-large-finetuned-race
- Architecture: RoBERTa with multiple-choice classification head
- Model Tag: roberta_race

## Data Source:
- Dataset: HellaSwag validation set (loaded from HuggingFace)
- Subset Size: 1000 samples (250 per class, balanced)
- Format: Multiple-choice questions with 4 options

## Calibration Configuration:
- Calibration Ratio: 20% (200 samples for calibration, 800 for testing)
- Temperature Scaling: Optimized on calibration set using grid search
- Optimization Metric: Negative Log-Likelihood (NLL)

## Pipeline Execution Steps:
1. Data Loading: Create balanced HellaSwag subset from HuggingFace
2. Model Scoring: Generate predictions for all samples (with caching)
3. Data Splitting: Stratified 20/80 split into CAL/TEST sets
4. Temperature Scaling: Find optimal temperature T on CAL set
5. Calibration Metrics: Compute Accuracy, NLL, ECE, Brier Score on TEST set
6. Risk-Coverage Analysis: Evaluate selective prediction performance
7. Visualization: Generate calibration and risk-coverage plots

## Dynamic Risk Targets:
Risk targets are automatically set based on model performance:
- Base risk = 1 - Accuracy (after temperature scaling)
- Range: [base_risk - 0.10, base_risk + 0.10]
- Number of targets: 5 (evenly spaced)

## Coverage Targets:
Fixed coverage levels for R@C analysis:
- 20%, 30%, 40%, 60%, 80%




"""

# =============================================================================
# MODEL EXECUTION: RoBERTa
# =============================================================================

print("\n" + "="*80)
print("EXECUTING: RoBERTa Model")
print("="*80)

roberta_config = {
    'type': 'roberta',
    'name_or_path': 'LIAMF-USP/roberta-large-finetuned-race',
    'tag': 'roberta'
}

CAL_RATIOS = [0.05, 0.10, 0.20, 0.40]

for cal_ratio in CAL_RATIOS:
    print(f"\n--- Running RoBERTa with CAL_RATIO = {cal_ratio} ---")

    roberta_outdir = os.path.join(RESULTS_DIR, roberta_config['tag'], f"cal{int(cal_ratio*100)}")
    os.makedirs(roberta_outdir, exist_ok=True)

    df_metrics_roberta, df_summary_roberta = run_full_pipeline(
        scorer_config=roberta_config,
        cal_ratio=cal_ratio,
        force_recreate_data=False,
        force_rescore=False,
        model_outdir=roberta_outdir
    )

print("\n" + "="*80)
print("RoBERTa Execution Complete for All Calibration Ratios")
print("="*80)

"""# DeBERTa Model Execution

This section executes the complete risk-aware calibration pipeline for the DeBERTa model with **custom risk and coverage targets**.

## Model Configuration:
- Model: sileod/deberta-v3-base-tasksource-nli
- Architecture: DeBERTa-v3-base with Natural Language Inference head
- Model Tag: deberta_nli
- Scoring Method: Entailment-based (premise-hypothesis pairs)

### Data Source:
Stays the Same

### Calibration Configuration:
Stays the Same

### Pipeline Execution Steps:
Stays the Same




"""

# =============================================================================
# MODEL EXECUTION: DeBERTa
# =============================================================================

if __name__ == "__main__":

    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(RESULTS_DIR, exist_ok=True)

    FORCE_RECREATE_DATA = False
    FORCE_RESCORE_MODEL = False

    MODEL_CONFIG = {
        "type": "deberta_nli",
        "name_or_path": "sileod/deberta-v3-base-tasksource-nli",
        "tag": "deberta_nli"
    }

    CAL_RATIOS = [0.05, 0.10, 0.20, 0.40]

    if not os.path.exists(DATA_PATHS[0]) and not os.path.exists(DATA_PATHS[1]):
        print("\nERROR: Dataset not found!")
        print("Please run the dataset creation script first.")
        print("Expected files:")
        for path in DATA_PATHS:
            print(f"  - {path}")
        exit(1)

    for CAL_RATIO in CAL_RATIOS:

        model_tag = MODEL_CONFIG['tag']
        model_outdir = os.path.join(RESULTS_DIR, model_tag, f"cal{int(CAL_RATIO*100)}")
        os.makedirs(model_outdir, exist_ok=True)

        print("\n" + "="*80)
        print(f"RUNNING DEBERTA PIPELINE (CAL_RATIO = {CAL_RATIO})")
        print("="*80)

        metrics_df, abstain_df = run_full_pipeline(
            scorer_config=MODEL_CONFIG,
            cal_ratio=CAL_RATIO,
            force_recreate_data=FORCE_RECREATE_DATA,
            force_rescore=FORCE_RESCORE_MODEL,
            model_outdir=model_outdir
        )

        print("\n" + "=" * 80)
        print(f"DEBERTA RUN COMPLETE. RESULTS SAVED TO: {model_outdir}")
        print("=" * 80)

        print("\n" + "=" * 60)
        print(f"DeBERTa Calibration Metrics (CAL_RATIO = {CAL_RATIO})")
        print("=" * 60)
        print(metrics_df.to_string())

        print("\n" + "=" * 60)
        print(f"DeBERTa Abstention Summary (CAL_RATIO = {CAL_RATIO})")
        print("=" * 60)
        print(abstain_df.to_string())

        print("\n" + "=" * 60)
        print("Generated Visualization Files:")
        print("=" * 60)
        print(f"  - {model_outdir}/calibration_scatter_{model_tag}.png")
        print(f"  - {model_outdir}/fig_calib_hist_{model_tag}.png")

    print("\n" + "="*80)
    print("DeBERTa Execution Complete for All Calibration Ratios")
    print("="*80)

"""# FLAN-T5 Model Execution

This section executes the complete risk-aware calibration pipeline for the FLAN-T5 model.

### Model Configuration:
- Model: google/flan-t5-xl
- Architecture: T5 encoder-decoder (3B parameters)
- Model Tag: flan_t5_xl
- Scoring Method: Teacher-forcing with multiple answer templates and few-shot prompting

### Data Source:
Stays the Same

### Calibration Configuration:
Stays the Same

### Pipeline Execution Steps:
Stays the Same

### Dynamic Risk Targets:
Stays the Same

### Coverage Targets:
Stays the Same


"""

# =============================================================================
# MODEL EXECUTION: FLAN-T5
# =============================================================================

if __name__ == "__main__":

    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(RESULTS_DIR, exist_ok=True)

    FORCE_RECREATE_DATA = False
    FORCE_RESCORE_MODEL = False

    MODEL_CONFIG = {
        "type": "t5",
        "name_or_path": "google/flan-t5-xl",
        "tag": "flan_t5_xl"
    }

    CAL_RATIOS = [0.05, 0.10, 0.20, 0.40]

    if not os.path.exists(DATA_PATHS[0]) and not os.path.exists(DATA_PATHS[1]):
        print("\nERROR: Dataset not found!")
        print("Please run the dataset creation script first.")
        print("Expected files:")
        for path in DATA_PATHS:
            print(f"  - {path}")
        exit(1)

    for CAL_RATIO in CAL_RATIOS:

        model_tag = MODEL_CONFIG['tag']
        model_outdir = os.path.join(RESULTS_DIR, model_tag, f"cal{int(CAL_RATIO*100)}")
        os.makedirs(model_outdir, exist_ok=True)

        print("\n" + "="*80)
        print(f"RUNNING FLAN-T5 PIPELINE (CAL_RATIO = {CAL_RATIO})")
        print("="*80)

        metrics_df, abstain_df = run_full_pipeline(
            scorer_config=MODEL_CONFIG,
            cal_ratio=CAL_RATIO,
            force_recreate_data=FORCE_RECREATE_DATA,
            force_rescore=FORCE_RESCORE_MODEL,
            model_outdir=model_outdir
        )

        print("\n" + "=" * 80)
        print(f"FLAN-T5 RUN COMPLETE. RESULTS SAVED TO: {model_outdir}")
        print("=" * 80)

        print("\n" + "=" * 60)
        print(f"FLAN-T5 Calibration Metrics (CAL_RATIO = {CAL_RATIO})")
        print("=" * 60)
        print(metrics_df.to_string())

        print("\n" + "=" * 60)
        print(f"FLAN-T5 Abstention Summary (CAL_RATIO = {CAL_RATIO})")
        print("=" * 60)
        print(abstain_df.to_string())

        print("\n" + "=" * 60)
        print("Generated Visualization Files:")
        print("=" * 60)
        print(f"  - {model_outdir}/calibration_scatter_{model_tag}.png")
        print(f"  - {model_outdir}/fig_calib_hist_{model_tag}.png")

    print("\n" + "="*80)
    print("FLAN-T5 Execution Complete for All Calibration Ratios")
    print("="*80)

"""# Phi-3.5 Model Execution

This section executes the complete risk-aware calibration pipeline for the Phi-3.5 model.

### Model Configuration:
- Model: microsoft/Phi-3.5-mini-instruct
- Architecture: Decoder-only transformer (3.8B parameters)
- Model Tag: phi_3.5_mini
- Scoring Method: Letter-only log-likelihood with few-shot prompting

### Data Source:
Stays the Same
### Calibration Configuration:
Stays the Same

### Pipeline Execution Steps:
Stays the Same

### Dynamic Risk Targets:
Stays the Same

### Coverage Targets:
Stays the Same

"""

# =============================================================================
# MODEL EXECUTION: Phi-3.5
# =============================================================================

if __name__ == "__main__":

    os.makedirs(DATA_DIR, exist_ok=True)
    os.makedirs(RESULTS_DIR, exist_ok=True)

    FORCE_RECREATE_DATA = False
    FORCE_RESCORE_MODEL = False

    MODEL_CONFIG = {
        "type": "phi",
        "name_or_path": "microsoft/Phi-3.5-mini-instruct",
        "tag": "phi_3.5_mini"
    }

    CAL_RATIOS = [0.05, 0.10, 0.20, 0.40]

    if not os.path.exists(DATA_PATHS[0]) and not os.path.exists(DATA_PATHS[1]):
        print("\nERROR: Dataset not found!")
        print("Please run the dataset creation script first.")
        print("Expected files:")
        for path in DATA_PATHS:
            print(f"  - {path}")
        exit(1)

    for CAL_RATIO in CAL_RATIOS:

        model_tag = MODEL_CONFIG['tag']
        model_outdir = os.path.join(RESULTS_DIR, model_tag, f"cal{int(CAL_RATIO*100)}")
        os.makedirs(model_outdir, exist_ok=True)

        print("\n" + "="*80)
        print(f"RUNNING PHI-3.5 PIPELINE (CAL_RATIO = {CAL_RATIO})")
        print("="*80)

        metrics_df, abstain_df = run_full_pipeline(
            scorer_config=MODEL_CONFIG,
            cal_ratio=CAL_RATIO,
            force_recreate_data=FORCE_RECREATE_DATA,
            force_rescore=FORCE_RESCORE_MODEL,
            model_outdir=model_outdir
        )

        print("\n" + "=" * 80)
        print(f"PHI-3.5 RUN COMPLETE. RESULTS SAVED TO: {model_outdir}")
        print("=" * 80)

        print("\n" + "=" * 60)
        print(f"Phi-3.5 Calibration Metrics (CAL_RATIO = {CAL_RATIO})")
        print("=" * 60)
        print(metrics_df.to_string())

        print("\n" + "=" * 60)
        print(f"Phi-3.5 Abstention Summary (CAL_RATIO = {CAL_RATIO})")
        print("=" * 60)
        print(abstain_df.to_string())

        print("\n" + "=" * 60)
        print("Generated Visualization Files:")
        print("=" * 60)
        print(f"  - {model_outdir}/calibration_scatter_{model_tag}.png")
        print(f"  - {model_outdir}/fig_calib_hist_{model_tag}.png")

    print("\n" + "="*80)
    print("Phi-3.5 Execution Complete for All Calibration Ratios")
    print("="*80)