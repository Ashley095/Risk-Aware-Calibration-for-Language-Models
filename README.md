# Risk-Aware-Calibration-for-Language-Models
This project implements temperature scaling calibration for large language models on the HellaSwag multiple-choice question answering task. We evaluate four different models (RoBERTa, DeBERTa, FLAN-T5, and Phi-3.5) across multiple calibration ratios to analyze the trade-off between prediction confidence and accuracy.
